import sqlite3
import pymysql
import time
import subprocess
import os
import signal
import sys
import logging
import requests
import json
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta, datetime

# Configuration
DB_TYPE = os.getenv("DB_TYPE", "mysql")
DB_HOST = os.getenv("DB_HOST", "mariadb")
DB_PORT = int(os.getenv("DB_PORT") or 3306)
DB_USER = os.getenv("DB_USER", "hfish") # User Request (Made configurable)
DB_PASSWORD = os.getenv("DB_PASSWORD", "password") # Default password for hfish
DB_NAME = os.getenv("DB_NAME", "hfish")

# Threat Intelligence Bridge Config
THREAT_BRIDGE_WEBHOOK_URL = os.getenv("THREAT_BRIDGE_WEBHOOK_URL")

# Legacy/SQLite Path
DB_PATH = "/hfish_ro/database/hfish.db"

SCANS_DIR = "/app/scans"
FEED_DIR = "/app/feed"
ASSETS_DIR = "/app/assets"
BANNED_IPS_FILE = os.path.join(FEED_DIR, "banned_ips.txt")
INDEX_FILE = os.path.join(FEED_DIR, "index.html")
LIVE_THREATS_FILE = os.path.join(ASSETS_DIR, "live_threats.json")
STATS_FILE = os.path.join(ASSETS_DIR, "stats.json")
REPORT_DIR = SCANS_DIR
scanning_ips = set() # Track IPs currently in queue or being scanned


MAX_WORKERS = 16  # Optimized concurrency (User Request: 16)

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("HoneySidecar")

def is_loopback(ip):
    """Check if the IP is a loopback address."""
    return ip in ("127.0.0.1", "::1", "localhost")

def signal_handler(sig, frame):
    logger.info("Exiting...")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Passwords
DEFAULT_PASS_HASH_OLD = "$2a$04$9PBC6S/jB8w4jUZcMkbSs.8TkraTZxUU8ZCk2HIXW1l2Q1dEH84gu" # HFish2021
DEFAULT_PASS_HASH_NEW = "$2y$04$qxgj8E6W/BhtiMmf4GO1t.2FsMD/96WYblQmGxaIko6P.0a9hIZsm" # HoneyScan2024!

def get_db_connection():
    try:
        if DB_TYPE.lower() in ("mysql", "mariadb"):
            # logger.info(f"Connecting with user={DB_USER}") # Reduce log noise
            return pymysql.connect(
                host=DB_HOST,
                port=DB_PORT,
                user=DB_USER,
                password=DB_PASSWORD,
                database=DB_NAME,
                cursorclass=pymysql.cursors.DictCursor,
                connect_timeout=10,
                autocommit=True,
                charset='utf8mb4'
            )
        else:
            return sqlite3.connect(f"file:{DB_PATH}?mode=ro", uri=True)
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        return None

def ensure_db_schema():
    if DB_TYPE.lower() not in ("mysql", "mariadb"):
        return
    conn = None
    try:
        conn = get_db_connection()
        if not conn: 
            logger.warning("Skipping schema migration - no DB connection")
            return
        cursor = conn.cursor()
        cursor.execute("DESCRIBE nodes")
        columns = [row['Field'] for row in cursor.fetchall()]
        alter_cmds = []
        if 'lat' not in columns: alter_cmds.append("ADD COLUMN lat FLOAT DEFAULT 0.0")
        if 'lng' not in columns: alter_cmds.append("ADD COLUMN lng FLOAT DEFAULT 0.0")
        if 'city' not in columns: alter_cmds.append("ADD COLUMN city VARCHAR(64) DEFAULT ''")
        if 'country' not in columns: alter_cmds.append("ADD COLUMN country VARCHAR(64) DEFAULT ''")
        for cmd in alter_cmds:
            logger.info(f"Migrating DB (nodes): {cmd}")
            cursor.execute(f"ALTER TABLE nodes {cmd}")
        
        # Check ipaddress table for pushed_to_bridge column
        cursor.execute("DESCRIBE ipaddress")
        ip_columns = [row['Field'] for row in cursor.fetchall()]
        if 'pushed_to_bridge' not in ip_columns:
            logger.info("Migrating DB (ipaddress): ADD COLUMN pushed_to_bridge")
            cursor.execute("ALTER TABLE ipaddress ADD COLUMN pushed_to_bridge TINYINT DEFAULT 0")
            cursor.execute("CREATE INDEX idx_pushed_to_bridge ON ipaddress(pushed_to_bridge)")

        # Fix "Data too long" for region
        try:
             cursor.execute("ALTER TABLE ipaddress MODIFY region VARCHAR(128) DEFAULT ''")
        except Exception as e:
             pass
            
        logger.info("Schema migration completed successfully")
    except Exception as e:
        logger.warning(f"Schema migration skipped due to error: {e}")
        # Non-fatal - allow monitor to continue
    finally:
        if conn: 
            try: 
                conn.close()
            except: 
                pass

def get_geolocation():
    try:
        ip = None
        for url in ["https://api.ipify.org?format=json", "https://api.myip.com"]:
            try:
                ip_resp = requests.get(url, timeout=5).json()
                ip = ip_resp.get('ip')
                if ip: break
            except:
                continue
        if not ip: 
            logger.error("Could not determine public IP after retries.")
            return None
        geo = requests.get(f"http://ip-api.com/json/{ip}", timeout=10).json()
        if geo.get('status') == 'success':
            return {
                'ip': ip,
                'lat': geo.get('lat', 0.0),
                'lng': geo.get('lon', 0.0),
                'city': geo.get('city', 'Unknown'),
                'country': geo.get('country', 'Unknown'),
                'country': f"{geo.get('city')}, {geo.get('country')}"
            }
        else:
            logger.warning(f"Geo API failed for {ip}: {geo.get('message')}")
    except Exception as e:
        logger.error(f"Geolocation fetch failed: {e}")
    return None

def update_node_location():
    # check_cloud_connectivity() removed 
    data = get_geolocation()
    if not data:
        logger.warning("Could not determine dynamic location.")
        return
    logger.info(f"Detected Public IP: {data['ip']}, Location: {data['country']} ({data['lat']}, {data['lng']})")
    try:
        json_path = "/app/assets/location.json"
        if not os.path.exists(os.path.dirname(json_path)):
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
        with open(json_path, 'w') as f:
            json.dump(data, f)
    except Exception as e:
        logger.error(f"Failed to write side-channel location: {e}")
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        if DB_TYPE == "mysql":
            query = """UPDATE nodes SET location = %s, lat = %s, lng = %s, city = %s, country = %s"""
            cursor.execute(query, (data['country'], data['lat'], data['lng'], data['city'], data['country']))
            if cursor.rowcount > 0: logger.info(f"Updated {cursor.rowcount} node(s) location.")
        conn.close()
    except Exception as e:
        logger.error(f"Database update failed: {e}")
        if conn: conn.close()

def push_intelligence(ip, is_new_hint=None):
    if not THREAT_BRIDGE_WEBHOOK_URL:
        return
    try:
        # Use simple logging if hint is not provided to keep logs clean
        if is_new_hint is None:
            logger.info(f"Pushing intelligence for {ip} to bridge...")
            
        resp = requests.post(
            THREAT_BRIDGE_WEBHOOK_URL,
            json={"attack_ip": ip},
            timeout=10
        )
        if resp.status_code == 200:
            try:
                data = resp.json()
                is_new = data.get("is_new")
                
                # Fallback to hint if response doesn't contain is_new (e.g. central bridge)
                if is_new is None:
                    is_new = is_new_hint
                
                if is_new is True:
                    logger.info(f"âœ… New IP added to bridge: {ip}")
                elif is_new is False:
                    logger.info(f"ðŸ”„ Updated intelligence for {ip} to bridge...")
                else:
                    logger.info(f"Intelligence push success for {ip}: {resp.text}")
            except Exception:
                 logger.info(f"Intelligence push success for {ip}: {resp.text}")
        else:
            logger.warning(f"Intelligence push failed for {ip}: HTTP {resp.status_code}")
    except Exception as e:
        logger.error(f"Error pushing intelligence for {ip}: {e}")
        return False
    return True

def sync_to_bridge():
    """Sync unsynced IPs to the bridge with a 50ms delay."""
    if not THREAT_BRIDGE_WEBHOOK_URL:
        return
    
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        # Fetch a batch of unsynced IPs
        cursor.execute("SELECT ip FROM ipaddress WHERE pushed_to_bridge = 0 AND ip NOT IN ('::1', '127.0.0.1', 'localhost') LIMIT 100")
        rows = cursor.fetchall()
        if not rows:
            return

        logger.info(f"Syncing {len(rows)} IPs to bridge...")
        for row in rows:
            ip = row['ip']
            # During full sync, they are technically 'new' for the bridge status check
            if push_intelligence(ip, is_new_hint=True):
                # Update status in DB
                cursor.execute("UPDATE ipaddress SET pushed_to_bridge = 1 WHERE ip = %s", (ip,))
            
            # Respect the 50ms delay requested by the user
            time.sleep(0.05)
            
        conn.commit()
    except Exception as e:
        logger.error(f"Sync to bridge failed: {e}")
    finally:
        if conn: conn.close()

# query_threatbook_ip removed

def update_threat_feed():
    logger.info("Updating Threat Feed...")
    conn = get_db_connection()
    if not conn: return
    recent_hackers = []
    suspicious_cs = []
    try:
        cursor = conn.cursor()
        # Pre-fetch known Fail2Ban IPs from the last 14 days to prioritize their metadata
        # preventing "Bridge Sync" from overriding the "Fail2Ban" status.
        f2b_ips = set()
        try:
            f2b_query = "SELECT DISTINCT source_ip FROM infos WHERE service = 'FAIL2BAN' AND create_time >= DATE_SUB(NOW(), INTERVAL 14 DAY)"
            cursor.execute(f2b_query)
            f2b_rows = cursor.fetchall()
            for r in f2b_rows:
                f2b_ip = r['source_ip'] if isinstance(r, dict) else r[0]
                f2b_ips.add(f2b_ip)
            logger.info(f"Loaded {len(f2b_ips)} Fail2Ban IPs for metadata prioritization.")
        except Exception as e:
            logger.warning(f"Failed to pre-fetch Fail2Ban IPs: {e}")

        cursor = conn.cursor()
        query = "SELECT DISTINCT source_ip, source_ip_country, create_time, service FROM infos ORDER BY create_time DESC LIMIT 162"
        logger.info(f"Executing Query: {query}")
        cursor.execute(query)
        rows = cursor.fetchall()
        logger.info(f"Fetched {len(rows)} rows from DB")
        for row in rows:
            ip = row['source_ip'] if isinstance(row, dict) else row[0]
            country = row.get('source_ip_country', 'Unknown') if isinstance(row, dict) else "Unknown"
            service_actual = row.get('service', '') if isinstance(row, dict) else ""
            service = service_actual
            
            # Metadata Override: If IP is a known Fail2Ban jail, force service context
            if ip in f2b_ips:
                service = 'FAIL2BAN'
            
            # Formatting for Fail2Ban entries
            threat_type = "Port Scanner"
            threat_risk = "Medium"
            location_disp = get_english_name(country)
            
            if service == 'FAIL2BAN':
                threat_type = "jailed by rules"
                threat_risk = "low"
                location_disp = "FAIL2BAN"
            elif service == 'BRIDGE_SYNC':
                threat_type = "Global Threat"
                threat_risk = "High"
                location_disp = "Honey Cloud"

            # Time Adjustment: DB is now Unified UTC (Native=UTC, Fail2Ban=UTC).
            # No shift needed. Frontend (UTC+1) will handle display.
            raw_time = row.get('create_time')
            adjusted_time = raw_time

            if len(recent_hackers) < 135:
                recent_hackers.append({
                    "ip": ip,
                    "location": location_disp,
                    "time": str(adjusted_time if adjusted_time else 'Just now'),
                    "flag": get_english_name(country) if service != 'FAIL2BAN' else "Unknown",
                    "count": 1
                })
            if len(suspicious_cs) < 130:
                suspicious_cs.append({
                        "ip": ip,
                        "location": location_disp,
                        "type": threat_type,
                        "risk": threat_risk,
                        "time": str(adjusted_time if adjusted_time else 'Just now')
                    })

        # Enforce exact limit of 130 items (26 * 5 pages)
        recent_hackers = recent_hackers[:130]
        suspicious_cs = suspicious_cs[:130]

        output = {"hackers": recent_hackers, "cs": suspicious_cs, "api_active": False}
        
        # Direct write to preserve inode for Docker bind mount
        with open(LIVE_THREATS_FILE, "w") as f:
            json.dump(output, f)
            f.flush()
            os.fsync(f.fileno())

        # Generate General Stats
        stats = {
            "total_attacks": 0,
            "today_attacks": 0,
            "top_country": "Unknown"
        }
        try:
             # Total
             cursor.execute("SELECT COUNT(*) FROM infos")
             row = cursor.fetchone()
             stats["total_attacks"] = row['COUNT(*)'] if isinstance(row, dict) else row[0]

             # Today (Approximation, assume timestamps are standard)
             # MariaDB/MySQL specific syntax
             cursor.execute("SELECT COUNT(*) FROM infos WHERE create_time >= CURDATE()")
             row = cursor.fetchone()
             stats["today_attacks"] = row['COUNT(*)'] if isinstance(row, dict) else row[0]

             # Top Country
             cursor.execute("SELECT country, COUNT(*) as c FROM ipaddress GROUP BY country ORDER BY c DESC LIMIT 1")
             row = cursor.fetchone()
             if row:
                 stats["top_country"] = row['country'] if isinstance(row, dict) else row[0]
        except Exception as e:
            logger.warning(f"Stats calculation partial failure: {e}")

        # Direct write for stats
        with open(STATS_FILE, "w") as f:
            json.dump(stats, f)
            f.flush()
            os.fsync(f.fileno())

    except Exception as e:
        logger.error(f"Error updating threat feed: {e}")
    finally:
        if conn: conn.close()

def get_new_attackers():
    conn = get_db_connection()
    if not conn: return []
    ips = []
    try:
        cursor = conn.cursor()
        # Filter for recent (last 14 days/336h) IPs to align with banned list policy and avoid reprocessing old IPs
        if DB_TYPE.lower() in ("mysql", "mariadb"):
            query = "SELECT DISTINCT ip FROM ipaddress WHERE create_time >= DATE_SUB(NOW(), INTERVAL 336 HOUR) ORDER BY create_time DESC LIMIT 7500"
        else:
            query = "SELECT DISTINCT ip FROM ipaddress WHERE create_time >= datetime('now', '-336 hours') ORDER BY create_time DESC LIMIT 7500"
        
        cursor.execute(query)
        rows = cursor.fetchall()
        for row in rows:
            ips.append(row['ip'] if isinstance(row, dict) else row[0])
        conn.close()
    except Exception as e:
        logger.error(f"Error fetching attackers: {e}")
        if conn: conn.close()
    
    # Check if force rescan is enabled
    force_rescan = os.getenv('FORCE_RESCAN', 'false').lower() == 'true'
    
    if force_rescan:
        # Skip ban check - scan everything without reports
        new_ips = []
        for ip in ips:
            if len(ip) < 7 or ip in scanning_ips or is_loopback(ip):
                continue
            report_path = os.path.join(REPORT_DIR, f"{ip}.txt")
            if not os.path.exists(report_path):
                new_ips.append(ip)
        if new_ips:
            logger.info(f"FORCE RESCAN: Found {len(new_ips)} IPs without reports (out of {len(ips)} total)")
        return new_ips
    
    # Normal mode: Filter out IPs already in banned list or currently scanning
    current_banned = set()
    if os.path.exists(BANNED_IPS_FILE):
        try:
            with open(BANNED_IPS_FILE, "r") as f:
                current_banned = set(line.strip() for line in f if line.strip())
        except Exception as e:
            logger.error(f"Error reading banned IPs: {e}")
    
    new_ips = [ip for ip in ips if ip not in current_banned and ip not in scanning_ips and len(ip) >= 7 and not is_loopback(ip)]
    if new_ips:
        logger.info(f"Found {len(new_ips)} new attacker IPs to process")
    return new_ips

def scan_ip(ip):
    report_path = os.path.join(REPORT_DIR, f"{ip}.txt")
    if os.path.exists(report_path): return None
    logger.info(f"Scanning {ip}...")
    try:
        command = ["nmap", "-A", "-T4", "-Pn", ip]  # Comprehensive scan: includes OS detection, version, scripts, and traceroute
        result = subprocess.run(command, capture_output=True, text=True, timeout=120)
        with open(report_path, "w") as f:
            f.write(f"Scan Time: {time.ctime()}\n")
            f.write(f"Target: {ip}\n")
            f.write("-" * 40 + "\n")
            f.write(result.stdout)
        return ip
    except Exception as e:
        logger.error(f"Error scanning {ip}: {e}")
        return None
    finally:
        if ip in scanning_ips:
            scanning_ips.remove(ip)

def update_banned_list():
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        # Regenerate banned list from DB (infos table) for the last 14 days (336 hours)
        # This ensures the list serves as a 14-day rolling window blocklist
        if DB_TYPE.lower() in ("mysql", "mariadb"):
            query = "SELECT DISTINCT source_ip FROM infos WHERE create_time >= DATE_SUB(NOW(), INTERVAL 336 HOUR)"
        else:
            query = "SELECT DISTINCT source_ip FROM infos WHERE create_time >= datetime('now', '-336 hours')"
            
        cursor.execute(query)
        rows = cursor.fetchall()
        
        banned_ips = set()
        for row in rows:
            ip = row['source_ip'] if isinstance(row, dict) else row[0]
            if not is_loopback(ip):
                banned_ips.add(ip)

        with open(BANNED_IPS_FILE, "w") as f:
            for ip in sorted(banned_ips):
                f.write(f"{ip}\n")
        logger.info(f"Updated banned list from DB (last 14 days). Total active: {len(banned_ips)}")
        
    except Exception as e:
        logger.error(f"Error updating banned list: {e}")
    finally:
        if conn: conn.close()


    
    # update_index() call removed (Generated by PHP now)



def fix_missing_severity():
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        if DB_TYPE == "mysql":
            # Fix empty severity and set threat_level=2 (Suspicious) for Low risk entries
            # This ensures HFish UI displays a badge instead of [Unk]
            # Catch level 0 and 1
            query = "UPDATE ipaddress SET severity = 'Low', threat_level = 2 WHERE severity IS NULL OR severity = '' OR threat_level <= 1"
            cursor.execute(query)
            if cursor.rowcount > 0: logger.info(f"Fixed missing severity/level for {cursor.rowcount} IPs.")
            conn.commit()
    except Exception as e:
        logger.error(f"Error fixing severity: {e}")
    finally:
        if conn: conn.close()



def fix_default_password():
    """Checks for default insecure password and updates it to new secure default."""
    if DB_TYPE.lower() not in ("mysql", "mariadb"):
        return

    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        # Check known weak hash
        query = "SELECT id, password FROM users WHERE username = 'admin'"
        cursor.execute(query)
        row = cursor.fetchone()
        
        if row:
            current_hash = row['password'] if isinstance(row, dict) else row[1].decode() if isinstance(row[1], bytes) else row[1]
            # Verify if it matches the known insecure hash
            if current_hash == DEFAULT_PASS_HASH_OLD:
                logger.warning("Detected insecure default password (HFish2021). Updating to HoneyScan2024!...")
                update_query = "UPDATE users SET password = %s WHERE username = 'admin'"
                cursor.execute(update_query, (DEFAULT_PASS_HASH_NEW,))
                conn.commit()
                logger.info("Admin password updated successfully.")
            elif current_hash == DEFAULT_PASS_HASH_NEW:
                # logger.info("Admin password is secure (HoneyScan2024!).")
                pass
            else:
                pass # User has set a custom password, do nothing
                
    except Exception as e:
        logger.error(f"Error checking default password: {e}")
    finally:
        if conn: conn.close()

# Translation Dictionary (Chinese -> English)
TRANSLATIONS = {
    "ç¾Žå›½": "United States",
    "ä¸­å›½": "China",
    "ä¿„ç½—æ–¯": "Russia",
    "å¾·å›½": "Germany",
    "è‹±å›½": "United Kingdom",
    "æ³•å›½": "France",
    "æ—¥æœ¬": "Japan",
    "éŸ©å›½": "South Korea",
    "å°åº¦": "India",
    "å·´è¥¿": "Brazil",
    "åŠ æ‹¿å¤§": "Canada",
    "æ¾³å¤§åˆ©äºš": "Australia",
    "æ„å¤§åˆ©": "Italy",
    "è¥¿ç­ç‰™": "Spain",
    "è·å…°": "Netherlands",
    "ç‘žå£«": "Switzerland",
    "ç‘žå…¸": "Sweden",
    "æŒªå¨": "Norway",
    "ä¸¹éº¦": "Denmark",
    "èŠ¬å…°": "Finland",
    "æ³¢å…°": "Poland",
    "åœŸè€³å…¶": "Turkey",
    "ä»¥è‰²åˆ—": "Israel",
    "æ²™ç‰¹é˜¿æ‹‰ä¼¯": "Saudi Arabia",
    "é˜¿è”é…‹": "United Arab Emirates",
    "æ–°åŠ å¡": "Singapore",
    "é©¬æ¥è¥¿äºš": "Malaysia",
    "æ³°å›½": "Thailand",
    "è¶Šå—": "Vietnam",
    "å°åº¦å°¼è¥¿äºš": "Indonesia",
    "è²å¾‹å®¾": "Philippines",
    "å·´åŸºæ–¯å¦": "Pakistan",
    "åˆšæžœå…±å’Œå›½": "Republic of the Congo",
    "åˆšæžœæ°‘ä¸»å…±å’Œå›½": "DR Congo",
    "å­ŸåŠ æ‹‰å›½": "Bangladesh",
    "å¢¨è¥¿å“¥": "Mexico",
    "é˜¿æ ¹å»·": "Argentina",
    "æ™ºåˆ©": "Chile",
    "å“¥ä¼¦æ¯”äºš": "Colombia",
    "å—éž": "South Africa",
    "åŸƒåŠ": "Egypt",
    "å°¼æ—¥åˆ©äºš": "Nigeria",
    "è‚¯å°¼äºš": "Kenya",
    "ä¹Œå…‹å…°": "Ukraine",
    "ç½—é©¬å°¼äºš": "Romania",
    "æ·å…‹": "Czech Republic",
    "åŒˆç‰™åˆ©": "Hungary",
    "å¥¥åœ°åˆ©": "Austria",
    "æ¯”åˆ©æ—¶": "Belgium",
    "è‘¡è„ç‰™": "Portugal",
    "å¸Œè…Š": "Greece",
    "çˆ±å°”å…°": "Ireland",
    "æ–°è¥¿å…°": "New Zealand",
    "é¦™æ¸¯": "Hong Kong",
    "å°æ¹¾": "Taiwan",
    "å±åœ°é©¬æ‹‰": "Guatemala",
    "æ¬§æ´²åœ°åŒº": "Europe Region",
    "äºšå¤ªåœ°åŒº": "Asia-Pacific Region",
    "éžæ´²åœ°åŒº": "Africa Region",
    "åŒ—ç¾Žåœ°åŒº": "North America Region",
    "æ‹‰ç¾Žåœ°åŒº": "Latin America Region",
    "ä¿åŠ åˆ©äºš": "Bulgaria",
    "æ‘©æ´›å“¥": "Morocco",
    "çˆ±æ²™å°¼äºš": "Estonia",
    "ä¼Šæœ—": "Iran",
    "ä¼Šæ‹‰å…‹": "Iraq",
    "ç«‹é™¶å®›": "Lithuania",
    "å“ˆè¨å…‹æ–¯å¦": "Kazakhstan",
    "é˜¿å¡žæ‹œç–†": "Azerbaijan",
    "çªå°¼æ–¯": "Tunisia",
    "ä¹Œå…¹åˆ«å…‹æ–¯å¦": "Uzbekistan",
    "å­ŸåŠ æ‹‰": "Bangladesh",
    "æ³¢æ–¯å°¼äºšå’Œé»‘å¡žå“¥ç»´é‚£": "Bosnia and Herzegovina",
    "å§”å†…ç‘žæ‹‰": "Venezuela",
    "å¡žå†…åŠ å°”": "Senegal",
    "åŸƒå¡žä¿„æ¯”äºš": "Ethiopia",
    "ç™½ä¿„ç½—æ–¯": "Belarus",
    "å®‰å“¥æ‹‰": "Angola",
    "æ‘©å°”å¤šç“¦": "Moldova",
    "è€æŒ": "Laos",
    "æ ¼é²å‰äºš": "Georgia",
    "çŽ»åˆ©ç»´äºš": "Bolivia",
    "æ´ªéƒ½æ‹‰æ–¯": "Honduras",
    "æ–¯æ´›ä¼å…‹": "Slovakia",
    "æ–¯é‡Œå…°å¡": "Sri Lanka",
    "å°¼æ³Šå°”": "Nepal",
    "ç´¢é©¬é‡Œ": "Somalia",
    "å·´æ‹‰åœ­": "Paraguay",
    "å¡å¡”å°”": "Qatar",
    "å¡žå°”ç»´äºš": "Serbia",
    "é˜¿æ›¼": "Oman",
    "é˜¿å°”åŠåˆ©äºš": "Algeria",
    "çº¦æ—¦": "Jordan",
    "å‰å°”å‰æ–¯æ–¯å¦": "Kyrgyzstan",
    "ç§˜é²": "Peru",
    "åŠ è“¬": "Gabon",
    "åŽ„ç“œå¤šå°”": "Ecuador",
    "å–€éº¦éš†": "Cameroon",
    "å±€åŸŸç½‘": "LAN",
    "ç§‘å¨ç‰¹": "Kuwait",
    "åˆ©æ¯”äºš": "Libya",
    "åŒ—é©¬å…¶é¡¿": "North Macedonia",
    "æ‹‰è„±ç»´äºš": "Latvia",
    "åœ­äºšé‚£": "Guyana",
    "æ–¯æ´›æ–‡å°¼äºš": "Slovenia",
    "å¤šå“¥": "Togo",
    "ç§‘ç‰¹è¿ªç“¦": "Ivory Coast",
    "å¢æ£®å ¡": "Luxembourg",
    "ç§‘ç´¢æ²ƒ": "Kosovo",
    "ä¹Œæ‹‰åœ­": "Uruguay",
    "ç¼…ç”¸": "Myanmar",
    "æ´¥å·´å¸ƒéŸ¦": "Zimbabwe",
    "ç‰¹ç«‹å°¼è¾¾å’Œå¤šå·´å“¥": "Trinidad and Tobago",
    "æŸ¬åŸ”å¯¨": "Cambodia",
    "è’™å¤": "Mongolia",
    "åŠ çº³": "Ghana",
    "ç‰™ä¹°åŠ ": "Jamaica",
    "äºšç¾Žå°¼äºš": "Armenia",
    "é»Žå·´å«©": "Lebanon",
    "é˜¿å°”å·´å°¼äºš": "Albania",
    "å…‹ç½—åœ°äºš": "Croatia",
    "å¦æ¡‘å°¼äºš": "Tanzania",
    "èŽ«æ¡‘æ¯”å…‹": "Mozambique",
    "å“¥æ–¯è¾¾é»ŽåŠ ": "Costa Rica",
    "å·´æ‹¿é©¬": "Panama",
    "èµžæ¯”äºš": "Zambia",
    "è‹é‡Œå—": "Suriname",
    "ä¹Œå¹²è¾¾": "Uganda",
    "å·´æž—": "Bahrain",
    "å™åˆ©äºš": "Syria",
    "å‰å¸ƒæ": "Djibouti",
    "å†°å²›": "Iceland",
    "ä¹Ÿé—¨": "Yemen",
    "å¤šç±³å°¼åŠ ": "Dominican Republic",
    "å¡žæµ¦è·¯æ–¯": "Cyprus",
    "èŽ±ç´¢æ‰˜": "Lesotho",
    "åšèŒ¨ç“¦çº³": "Botswana",
    "æ ¹è¥¿å²›": "Guernsey",
    "æœ¬æœºåœ°å€": "Localhost",
    "æœªçŸ¥": "Unknown",
    "HFish Honeypot": "Honey Cloud",
    "hfish honeypot": "Honey Cloud"
}

def get_english_name(chinese_name):
    if not chinese_name:
        return "Unknown"
    
    # Clean string and ensure it is unicode
    name = str(chinese_name).strip()
    
    # Specific common patterns using unicode escapes for stability
    # \u53f0\u6e7e = å°æ¹¾, \u9999\u6e2f = é¦™æ¸¯, \u6fb3\u95e8 = æ¾³é—¨
    if "\u53f0\u6e7e" in name or "Taiwan" in name:
        return "Taiwan"
    if "\u9999\u6e2f" in name or "Hong Kong" in name:
        return "Hong Kong"
    if "\u6fb3\u95e8" in name or "Macau" in name:
        return "Macau"
        
    # If it contains China (\u4e2d\u56fd) in any form
    if "\u4e2d\u56fd" in name or "china" in name.lower():
        # Try to find a more specific translation for the sub-part if it's a hyphenated string
        if "-" in name:
            parts = name.split("-")
            for part in reversed(parts):
                p = part.strip()
                if p in TRANSLATIONS and p != "\u4e2d\u56fd":
                     return TRANSLATIONS[p]
        return "China"
    
    # Try direct mapping
    if name in TRANSLATIONS:
        return TRANSLATIONS[name]
        
    # Final check for sub-parts of any hyphenated string
    if "-" in name:
        parts = name.split("-")
        for part in reversed(parts):
            p = part.strip()
            if p in TRANSLATIONS:
                return TRANSLATIONS[p]
                
    # Return as-is if no match
    return name

def restore_db_language():
    """Revert English location names to Chinese in DB to fix HFish dashboard"""
    # OPTIMIZATION: Check if there are ANY English names before hammering the DB
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        
        # Check for presence of key English country names (Sample check)
        check_query = "SELECT COUNT(*) FROM ipaddress WHERE country IN ('United States', 'China', 'Germany', 'Russia', 'France')"
        cursor.execute(check_query)
        count = cursor.fetchone()[0]
        
        if count == 0:
            # DB seems clean, skip massive update
            return

        logger.info(f"Found {count} English entries. Restoring specific entries to Chinese...")

        if DB_TYPE == "mysql":
            # Create reverse mapping
            reverse_translations = {v: k for k, v in TRANSLATIONS.items()}
            
            # Optimized: Loop through reverse map, but maybe still heavy if done blindly? 
            # Better: Only update rows that MATCH the English name.
            # The previous code did "WHERE country = %s" which is fine, but doing it for 150+ countries is 150*3 queries.
            # We can rely on the fact that we only really need to fix the ones that are broken.
            # But "restore all" is safer.
            # Let's throttle this mechanism to run only once every 60 seconds?
            # Implemented via caller or just rely on the 'sample check' above to short-circuit.
            
            for en, cn in reverse_translations.items():
                # Only update if it currently matches the English name
                # This is still many queries, but standard for this script structure. 
                # The 'check_query' above saves us 99% of the time.
                cursor.execute("UPDATE ipaddress SET country = %s WHERE country = %s", (cn, en))
                # optimize: regions are less critical but good to fix
                # cursor.execute("UPDATE ipaddress SET region = %s WHERE region = %s", (cn, en)) 
                # optimize: infos table is huge, updating it every time is heavy. 
                # We mainly care about 'ipaddress' for the map. 
                # HFish likely uses 'ipaddress' for the map.
                
            conn.commit()
            logger.info("Restored DB location names to Chinese")
    except Exception as e:
        logger.warning(f"Restoration error: {e}")
    finally:
        if conn: conn.close()

def fix_unknown_countries():
    """Resolves 'Unknown' countries for IPs in the DB using background GeoIP lookup."""
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        # Fetch a few IPs with Unknown country
        cursor.execute("SELECT DISTINCT ip FROM ipaddress WHERE country = 'Unknown' AND ip NOT IN ('::1', '127.0.0.1', 'localhost') LIMIT 2")
        rows = cursor.fetchall()
        
        if not rows: return

        for row in rows:
            ip = row['ip'] if isinstance(row, dict) else row[0]
            logger.info(f"Resolving location for {ip}...")
            
            try:
                geo = requests.get(f"http://ip-api.com/json/{ip}", timeout=5).json()
                if geo.get('status') == 'success':
                    country = geo.get('country', 'Unknown')
                    city = geo.get('city', 'Unknown')
                    region = geo.get('regionName', '')

                    # Truncate to avoid "Data too long" errors
                    country = country[:64]
                    city = city[:64]
                    region = region[:128]
                    
                    # Update ipaddress table
                    cursor.execute("UPDATE ipaddress SET country = %s, city = %s, region = %s WHERE ip = %s", 
                                   (country, city, region, ip))
                                   
                    # Update infos table for historical accuracy in feed
                    cursor.execute("UPDATE infos SET source_ip_country = %s WHERE source_ip = %s AND source_ip_country = 'Unknown'", 
                                   (country, ip))
                    
                    logger.info(f"Resolved {ip} -> {country}")
                else:
                    pass
            except Exception as e:
                logger.warning(f"Geo lookup failed for {ip}: {e}")
            
            # Respect rate limit (45/min -> ~1.3s per req). 
            time.sleep(1.5)
            
        conn.commit()
    except Exception as e:
        logger.error(f"Error fixing unknown countries: {e}")
    finally:
        if conn: conn.close()

def init_env():
    if not os.path.exists(SCANS_DIR): os.makedirs(SCANS_DIR)
    if not os.path.exists(FEED_DIR): os.makedirs(FEED_DIR)
    if not os.path.exists(ASSETS_DIR): os.makedirs(ASSETS_DIR)
    # update_index() removed

def reset_sync_status():
    """Reset pushed_to_bridge status for all IPs on startup."""
    conn = get_db_connection()
    if not conn: return
    try:
        cursor = conn.cursor()
        logger.info("Resetting sync status for all IPs...")
        cursor.execute("UPDATE ipaddress SET pushed_to_bridge = 0")
        logger.info(f"Reset sync status for {cursor.rowcount} rows.")
        conn.commit()
    except Exception as e:
        logger.error(f"Failed to reset sync status: {e}")
    finally:
        if conn: conn.close()


def main():
    init_env()
    logger.info(f"Monitor started (DB_TYPE={DB_TYPE}).")
    logger.info(f"DEBUG: DB_USER={DB_USER}, DB_HOST={DB_HOST}, DB_NAME={DB_NAME}")

    if THREAT_BRIDGE_WEBHOOK_URL:
        logger.info(f"Threat Intelligence Bridge ENABLED. Target: {THREAT_BRIDGE_WEBHOOK_URL}")
    else:
        logger.warning("Threat Intelligence Bridge DISABLED (URL not set).")

    logger.info("Waiting 30s for DB to be ready...")
    time.sleep(30) 
    ensure_db_schema()
    # update_node_location()
    logger.info("Schema migration completed - proceeding to main loop")
    
    # Reset sync status on startup (REMOVED: Causes flooding/stagnation)
    # reset_sync_status()
    
    # Fix insecure default password
    fix_default_password()
    
    executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
    
    last_maintenance = 0
    
    try:
        while True:
            try:
                attackers = get_new_attackers()
                if attackers:
                    logger.info(f"Processing {len(attackers)} new attacker IPs...")
                    for ip in attackers:
                        scanning_ips.add(ip)
                        executor.submit(scan_ip, ip)
                        # Push to Threat Intelligence Bridge
                        # Check if it was already synced to decide on the hint
                        executor.submit(push_intelligence, ip, is_new_hint=True)
                
                # Optimization: Run heavy tasks roughly every 60s
                # Using timestamp check is more robust than modulo 0 against loop drift
                if time.time() - last_maintenance > 60:
                    update_banned_list()
                    fix_missing_severity()
                    last_maintenance = time.time()

                # Run background tasks
                fix_unknown_countries()
                restore_db_language()
                update_threat_feed()
                if int(time.time()) % 600 < 15: 
                    try:
                        update_node_location()
                    except Exception as e:
                        logger.warning(f"Node location update failed: {e}")
                
                # Run sync to bridge in the main loop
                sync_to_bridge()

            except Exception as e:
                logger.error(f"Loop error: {e}")
            time.sleep(10)
    finally:
        executor.shutdown(wait=False)

if __name__ == "__main__":
    main()
